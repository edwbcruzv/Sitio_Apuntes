<%- include ../layouts/cabecera.ejs %>

  <main>
    <h1>
      Regresión</h1>
    <h2>Lineal</h2>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_904af5e1a7806606.png" name="Imagen 1060857300" align="bottom"
        width="199" height="480" border="0" />
    </p>
    <p>Descripción:
      contiene información de algunos empleados como lo son los años de
      experiencia y el salario.
    </p>
    <p>Objetivo:
      Predecir el salario que pueda ganar un empleado ingresando los años
      de experiencia</p>
    <p><br />
      <br />

    </p>
    <h3>Python</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      los módulos necesarios y las sentencias para la lectura de datos,
      tomando las variables independientes y dependientes.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import numpy as np</span>
          </p>
          <p>
            <span>import matplotlib.pyplot as plt</span>
          </p>
          <p>
            <span>import pandas as pd</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>dataset = pd.read_csv('Salary_Data.csv')</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            X = dataset.iloc[:,:-1].values
          </p>
          <p>y =
            dataset.iloc[:,1].values</p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>Tratamiento de NAs</h4>
    <p>Como
      el dataset a simple vista no tiene NAs podemos omitir este paso.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>La
      variable independiente no es necesario escalarla al tratarse de un
      flotante y además por la descripción del problema no hay necesidad
      de buscarle categorías a la variable independiente.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>Dividimos
      el dataset, para testing destinamos una tercera parte en el argumento
      <b>size</b> y el resto se va a entrenamiento. Para llegar los mismos
      resultados de este tutorial puedes agregar el argumento <b>state = 5,
      </b>si no lo puedes omitir o poner números diferentes.
    </p>
    <table>
      <col width="581" />

      <tr>
        
          <p>
            <span>from sklearn.model_selection import
              train_test_split</span>
          </p>
          <p>

          </p>
          <p>
            <font size="2" style="font-size: 11pt"><span>X_train,
                X_test, y_train, y_test =
                train_test_split(X,y,test_size=1/3,random_state=5)</span></font>
          </p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_c8a4e36654e64179.png" name="Imagen 1786042815" align="bottom"
        width="481" height="288" border="0" />
    </p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario realizar un escalado de variables al ser pocos datos.</p>
    <h4>Crear un modelo de Regresión Lineal Simple con
      el conjunto de entrenamiento</h4>
    <p>Importamos
      el módulo que crea el modelo de Regresión Lineal, solo al momento
      de mandar a ejecutar el<b> fit</b> mandamos como parámetros los
      conjuntos de entrenamiento independiente y dependiente
      respectivamente.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.linear_model import
              LinearRegression</span>
          </p>
          <p>

          </p>
          <p>
            <span>regression = LinearRegression()</span>
          </p>
          <p><span>regression.fit(X_train
              ,y_train)</span></p>
        
      </tr>
    </table>
    <h4>Predicción con el conjunto de test</h4>
    <p>En
      <b>y_pred</b> obtendremos el modelo lineal en base al conjunto de
      test.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred = regression.predict(X_test)</span>
          </p>
        
      </tr>
    </table>
    <p>Al
      compararla con el conjunto de real de test deberán de ser casi
      similares.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_c87ced3bec36367d.png" name="Imagen 2014687285" align="bottom"
        width="480" height="357" border="0" />
    </p>
    <h4>Visualizar los resultado del entrenamiento</h4>
    <p>Mostraremos
      una gráfica a partir de <b>matplotlib</b> para ver la línea recta
      que genero nuestro modelo, en base al conjunto de entrenamiento.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Dibujando los puntos del conjunto de entrenamiento de color
            rojo. plt.scatter(X_train,y_train, color=&quot;red&quot;)</p>
          <p>
            # Dibujando la linea de regresion de color azul</p>
          <p>
            <span>plt.plot(X_train,regression.predict(X_train),color=&quot;blue&quot;)</span>
          </p>
          <p>
            # Dando etiquetas a la grafica</p>
          <p>
            plt.title(&quot;Sueldo vs Años de experiencia(Entrenamiento)&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo ($)&quot;)</p>
          <p>
            plt.xlabel(&quot;Anos de experiencia&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_741e235b3473367d.png" name="Imagen 446297196" align="bottom"
        width="460" height="314" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Visualizar lo resultados de test</h4>
    <p>Mostraremos
      una gráfica a partir de <b>matplotlib</b> para ver la linea recta
      que genero nuestro modelo, en base al conjunto de test.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Dibujando los puntos del conjunto de testing de color rojo.</p>
          <p>
            <span>plt.scatter(X_test,y_test, color=&quot;red&quot;)</span>
          </p>
          <p>
            # Dibujando la linea de regresion de color azul</p>
          <p>
            <span>plt.plot(X_train,regression.predict(X_train),color=&quot;blue&quot;)</span>
          </p>
          <p>
            # Dando etiquetas a la grafica</p>
          <p>
            plt.title(&quot;Sueldo vs Años de experiencia(Testing)&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo ($)&quot;)</p>
          <p>
            plt.xlabel(&quot;Anos de experiencia&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <h3><br />

    </h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_37e2eb2df8b26a25.png" name="Imagen 1250477864" align="bottom"
        width="407" height="278" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h3>R</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('Salary_Data.csv')</span>
          </p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>Como
      el dataset a simple vista no tiene NAs podemos omitir este paso.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>La
      variable independiente no es necesario escalarla al tratarse de un
      flotante y además por la descripción del problema no hay necesidad
      de buscarle categorías a la variable independiente.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>Nos
      aseguramos de tener caTools instalado, sino descomentar las líneas
      de código y ejecuta. Si tienes problemas en instalar un repositorio
      desde R ve el siguiente tutorial.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            install.packages(&quot;caTools&quot;) # solo se necesita ejecutar
            una vez</p>
          <p>library(caTools)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p>Dividimos
      el dataset, para testing destinamos una tercera parte y el resto se
      va a entrenamiento dejando el argumento <b>SplitRatio = 2/3 </b> y.
      Para llegar los mismos resultados de este tutorial puedes agregar el
      argumento <b>set.seed(0) , </b>si no lo puedes omitir o poner números
      diferentes.
    </p>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # configurando semilla aleatoria para la division de datos</p>
          <p>
            set.seed(0)</p>
          <p>
            # se elige el porcentaje de los datos para el training en %</p>
          <p>
            # se selecciona la columna de la variable dependiente o a predecir</p>
          <p>
            <span>split = sample.split(dataset$Salary,SplitRatio
              = 2/3)</span>
          </p>
          <p>
            print(split)</p>
          <p>
            # Dividiendo el conjunto , True para el training</p>
          <p>
            <span>training_set = subset(dataset,split == TRUE)</span>
          </p>
          <p>
            # Dividiendo el conjunto , False para el test</p>
          <p><span>testing_set
              = subset(dataset,split == FALSE)</span></p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario realizar un escalado de variables al ser pocos datos.</p>
    <h4>Crear un modelo de Regresion Lineal Simple con el
      conjunto de entrenamiento</h4>
    <p>Mandamos
      a llamar a la función <b>lm</b> y la formula que escribimos quiere
      decir que el <b>Salary</b> está dada por <b>YearsExperience</b> y en
      <b>data </b>ponemos el conjunto de entrenamiento
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regressor=lm(formula=Salary ~
              YearsExperience,data=training_set)</span>
          </p>
          <p>summary(regressor)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Predecir con el conjunto de test</h4>
    <p>En
      y_pred obtendremos el modelo lineal en base al conjunto de test.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred=predict(regressor,newdata=testing_set)</span>
          </p>
          <p>
            print(y_pred)</p>
          <p><br />

          </p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Visualizar los resultados del entrenamiento</h4>
    <p>Usando
      la librería ggplot.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dibujando los puntos de entrenamiento</p>
          <p>

            geom_point(aes(x=training_set$YearsExperience,y=training_set$Salary),</p>
          <p>
            colour=&quot;red&quot;)+</p>
          <p>
            # Dibujando la linea de la predicción, en base al entrenamiento</p>
          <p>

            <span
             >geom_line(aes(x=training_set$YearsExperience,y=predict(regressor,newdata=training_set)),</span>
          </p>
          <p>
            <span>colour=&quot;blue&quot;)+</span>
          </p>
          <p>
            ggtitle(&quot;Sueldo vs Años de experiencia(Entrenamiento)&quot;)+</p>
          <p>
            xlab(&quot;Sueldo ($)&quot;)+</p>
          <p>
            ylab(&quot;Anos de experiencia&quot;)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Visualizar lo resultados de test</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de testing</p>
          <p>

            geom_point(aes(x=testing_set$YearsExperience,y=testing_set$Salary),</p>
          <p>
            colour=&quot;red&quot;)+</p>
          <p>
            # Dobujando la linea de la prediccion, en base al testing</p>
          <p>
            <span>geom_line(aes(x=testing_set$YearsExperience,y=y_pred),</span>
          </p>
          <p>
            <span>colour=&quot;blue&quot;)+</span>
          </p>
          <p>
            ggtitle(&quot;Sueldo vs Años de experiencia(testing)&quot;)+</p>
          <p>
            xlab(&quot;Sueldo ($)&quot;)+</p>
          <p>
            ylab(&quot;Anos de experiencia&quot;)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h2>Múltiple</h2>
    <p><br />
      <br />

    </p>
    <p>Debemos
      de recordar que no todas las variables son útiles ya que algunas
      pueden hacer lo mismo y entre menos variables mejor.</p>
    <p>No
      por agregar más variables tendremos más información ni mejor
      predicción, esto se hace más pesado y complicado de entender</p>
    <p>Métodos
      para construir modelos:</p>
    <ul>
      <li>
        <p>Exhaustivo
          (All-In)</p>
      <li>
        <p>Eliminación
          hacia atrás</p>
      <li>
        <p>Selección
          hacia adelante</p>
      <li>
        <p>Eliminación
          Bidireccional</p>
      <li>
        <p>Comparación
          de scores</p>
    </ul>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_590cb0322854021.png" name="Imagen 1672638227" align="bottom"
        width="480" height="281" border="0" />
    </p>
    <p>Descripción:
      Contiene información de 50 empresas con variables de gastos de dos
      áreas en la empresa y la ubicación geográfica, en la imagen solo
      se muestran las primeras 20. Esta tabla es de una empresa que se
      dedica a realizar préstamos a dichas empresas.</p>
    <p>Objetivo:
      La empresa quiere saber que beneficios obtiene del préstamo en base
      a sus gastos y la ubicación de la empresa.</p>
    <h3>Python</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      los módulos necesarios y las sentencias para la lectura de datos,
      tomando las variables independientes y dependientes.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <font color="#000000"><span>import numpy as np</span></font>
          </p>
          <p>
            <font color="#000000"><span>import matplotlib.pyplot
                as plt</span></font>
          </p>
          <p>
            <font color="#000000"><span>import pandas as pd</span></font>
          </p>
          <p>
            <br />

          </p>
          <p>
            <font color="#000000"><span>dataset =
                pd.read_csv('50_Startups.csv')</span></font>
          </p>
          <p>
            <font color="#000000">X = dataset.iloc[:,:-1].values</font>
          </p>
          <p>
            <font color="#000000">y
              = dataset.iloc[:,4].values</font>
          </p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      hay existen datos faltantes en el dataset.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>La
      columna “state” se debe de categorizar</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_6a767e135b354ff3.png" name="Imagen 329654106" align="bottom"
        width="480" height="450" border="0" />
    </p>
    <p>Seleccionamos
      la columna 3 para categorizarla con los módulos <b>LabelEncoder</b>
      y <b>OneHotEncoder</b>, leer documentación de su uso</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.preprocessing import LabelEncoder,
              OneHotEncoder</span>
          </p>
          <p>
            from sklearn.compose import ColumnTransformer</p>
          <p>
            <br />

          </p>
          <p>
            le_X =LabelEncoder()</p>
          <p>
            <br />

          </p>
          <p>X[:,3]=le_X.fit_transform(X[:,3])</p>
        
      </tr>
    </table>
    <p>Hasta
      este punto se sustituyeron los estados por números, siendo 0 para
      California, 1 para Florida y 2 para New York. Se sobrescribe la
      variable X.</p>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d3f3f873e9acf8df.png" name="Imagen 375157138" align="bottom"
        width="480" height="450" border="0" />
    </p>
    <p>Crearemos
      una columna por cada estado y por un booleano se sabrá el estado
      correspondiente.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            ct = ColumnTransformer(</p>
          <p>
            # Lista de tuplas (nombre,transformador,columnas) que se le
            aplicara
          </p>
          <p>
            # al conjunto de datos.</p>
          <p>

            [('one_hot_encoder',OneHotEncoder(categories='auto',dtype=int),[3])],</p>
          <p>
            # Se pasa el resto de columnas que no se tocaron.</p>
          <p>
            <span>remainder='passthrough')</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>X=ct.fit_transform(X)</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>X = np.array(X,dtype=float)</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            X=X[:,1:]</p>
          <p><br />

          </p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p>Estas
      columnas se paran a la primera columna.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_a0836d445ea888f3.png" name="Imagen 1406888151" align="bottom"
        width="480" height="409" border="0" />
    </p>
    <p>Para
      evitar caer en la trampa de las variables Dummy, eliminaremos la
      primera columna y conservaremos el resto.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            X=X[:,1:]</p>
        
      </tr>
    </table>
    <p>Recordando
      que cuando en ambas columnas categóricas sean 0 significa que es la
      categoría eliminada.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_43490f4e7491593f.png" name="Imagen 257353492" align="bottom"
        width="480" height="409" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>Dividimos
      el dataset 20% a test y el resto a entrenamiento, la semilla puede
      ser arbitraria.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.model_selection import
              train_test_split</span>
          </p>
          <p>
            <br />

          </p>
          <p><span>X_train,
              X_test, y_train, y_test =
              train_test_split(X,y,test_size=0.2,random_state=0)</span></p>
        
      </tr>
    </table>
    <p>Al
      ser datos solo se mostrará la cantidad de tuplas divididas.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_f722ceed9df99998.png" name="Imagen 872201924" align="bottom"
        width="450" height="193" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Escalado de variables</h4>
    <p>No
      se necesita escalado de variables.</p>
    <h4>Crear un modelo de Regresion Lineal Múltiple
      con el conjunto de entrenamiento</h4>
    <p>Se
      calcula la regresión múltiple enviando el conjunto de entrenamiento
      y de test.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.linear_model import
              LinearRegression</span>
          </p>
          <p>
            <span>regression = LinearRegression()</span>
          </p>
          <p><span
             >regression.fit(X_train,y_train)</span></p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>Predecir con el conjunto de test</h4>
    <p>Veremos
      que tanto se acercan a los datos originales.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred=regression.predict(X_test)</span>
          </p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_ad57bdf50247c7e9.png" name="Imagen 1043637061" align="bottom"
        width="480" height="401" border="0" />
    </p>
    <h3>Ajustando el modelo de Regresion Lineal Múltiple
      en Python (Eliminación hacia atrás)</h3>
    <p>Anteriormente
      se usaron todas las variables independientes para ajustar el modelo,
      ahora empezaremos a descartar algunas variables y podríamos
      conseguir un mejor resultado.</p>
    <p>No
      todas las variables pueden tener un impacto grande en la prediccion,
      debemos eliminar las menos significativas. Tenemos que encontrar el
      conjunto de variables independientes más significativas.</p>
    <p>Usaremos
      la librería <b>statsmodels</b> para la eliminación hacia atrás
      automáticamente. Nos ayudara el agregar y quitar modelos.</p>
    <p>Agregaremos
      una columna a la matriz agregando una columna, esa columna será
      nuestra variable independiente y nos ayudará a calcular el p-valor.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_870332e0c16c6a57.png" name="Imagen 13301411" align="bottom"
        width="480" height="395" border="0" />
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import statsmodels.api as sm</span>
          </p>
          <p>
            <br />

          </p>
          <p><span
             >X=np.append(arr=np.ones((50,1),dtype=int),values=X,axis=1)</span></p>
        
      </tr>
    </table>
    <p>La
      nueva columna de puso al principio de toda la matriz recorriendo el
      resto</p>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_956d8edfd12fd769.png" name="Imagen 808377510" align="bottom"
        width="480" height="395" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>PASO 1: SELECCIONAR EL NIVEL DE SIGNIFICACION</h4>
    <p>Definimos
      el nivel de significación, normalmente es el 0.05.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            SL=0.05</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4><br />

    </h4>
    <h4>PASO 2: SE CALCULA EL MODELO CON TODAS LAS
      POSIBLES VARIABLES PREDICTORAS</h4>
    <p>En
      la variable <b>X_opt </b>guardará el numero óptimos, será nuestra
      matriz de características óptimas. Se quedarán las variables
      independientes estadísticamente significativas que ayuden a predecir
      la variable independiente.</p>
    <p>Al
      principio empezaremos con todas las variables y a cada pase de la
      eliminación ira desapareciendo.</p>
    <p>Recordamos
      que se usa el método de los Mínimos Cuadrados Ordinarios (OLS).</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>X_opt=X[:,[0,1,2,3,4,5]]</span>
          </p>
          <p><span
             >regression_OLS=sm.OLS(y,X_opt).fit()</span></p>
        
      </tr>
    </table>
    <p>Enviamos
      todas las variables a predecir y mandamos las columnas que forman
      parte de las variables regresoras, en este caso son todas las
      columnas. La documentación indica que las variables tienen que ir
      con número y en una lista.</p>
    <p>Con
      <b>fit()</b> se hace el ajuste.
    </p>
    <h4><br />

    </h4>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Este
      paso nos da la siguiente información:</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            print(regression_OLS.summary())</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_fdf4ef8c55b841c5.png" name="Imagen 1265704454" align="bottom"
        width="480" height="373" border="0" />
    </p>
    <p>Nos
      interesan la información de las constantes la columna P&gt;|t| es
      que el P-valor</p>
    <p>A
      simple vista todos los coeficientes superan SL.</p>
    <p>Debemos
      de bajar p-valor para que la predicción sea más acertada, en este
      caso x3 es la variable más significativa.</p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      x3 &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Quitamos
      el coeficiente con el p-valor más alto eliminando el 3 de la
      variable <b>X_opt</b>.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            X_opt=X[:,[0,1,2,4,5]]</p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <p>Ajustamos
      el modelo.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression_OLS=sm.OLS(y,X_opt).fit()</span>
          </p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      lo resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            print(regression_OLS.summary())</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d5d4c91655f2f974.png" name="Imagen 689934122" align="bottom"
        width="480" height="373" border="0" />
    </p>
    <p>La
      etiqueta que se les da a las constantes se sobrescribe, pero en la
      matriz <b>X_opt </b>no, debemos tener cuidado que variables
      eliminamos más adelante.</p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      Al mostrar con sumary vemos que x2 &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Quitamos
      el coeficiente con el p-valor más alto eliminando el 2 de la
      variable <b>X_opt</b>.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            X_opt=X[:,[0,1,4,5]]</p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <p>Ajustamos
      el modelo.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression_OLS=sm.OLS(y,X_opt).fit()</span>
          </p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            print(regression_OLS.summary())</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_c045fd6435b0372b.png" name="Imagen 437339025" align="bottom"
        width="480" height="351" border="0" />
    </p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      Al mostrar con sumary vemos que x3 &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <p><br />
      <br />

    </p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Quitamos
      el coeficiente con el p-valor más alto eliminando el 5 de la
      variable X_opt.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            X_opt=X[:,[0,1,4]]</p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <p>Ajustamos
      el modelo.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression_OLS=sm.OLS(y,X_opt).fit()</span>
          </p>
        
      </tr>
    </table>
    <h4><br />

    </h4>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            print(regression_OLS.summary())</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_96ede21a393486a0.png" name="Imagen 647540666" align="bottom"
        width="480" height="344" border="0" />
    </p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      Al mostrar con sumary vemos no hay variables &gt; SL</p>
    <p>SINO
      AL FIN.</p>
    <h4>Conclusión</h4>
    <p>AL
      no existir un p-valor mayor a SL decimos que las columnas
      correspondientes a x1 y x2 que son la columna 1 y 4 son las más
      significativas y con las cuales podemos trabajar y dejar a un lado el
      resto de las columnas.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_cc7f8b4777ebb0f4.gif" name="Shape1" alt="Shape1" align="bottom"
        width="480" height="395" />
    </p>
    <h3>R</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('50_Startups.csv')</span>
          </p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_6aa3828d9e6c4447.png" name="Imagen 780373200" align="bottom"
        width="468" height="480" border="0" />
    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      es necesario al no existir NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>Codificamos
      la columna<b> state </b>de cadenas a números.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset$State = factor(dataset$State,</span>
          </p>
          <p>
            <span>levels = c(&quot;New
              York&quot;,&quot;California&quot;, &quot;Florida&quot;),</span>
          </p>
          <p>
            <span>labels = c(1,2,3))</span>
          </p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_df8f01b73c435048.png" name="Imagen 16847085" align="bottom"
        width="468" height="480" border="0" />
    </p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>Hacemos
      uso de la librería <b>caTools </b>para poder obtener números
      aleatorios y asi dividir nuestros conjuntos de testing y de
      entrenamiento.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # install.packages(&quot;caTools&quot;) # solo se necesita
            ejecutar una vez</p>
          <p>
            <span>library(caTools) </span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>set.seed(123)</span>
          </p>
          <p>
            <span>split = sample.split(dataset$Profit,SplitRatio
              = 0.8)</span>
          </p>
          <p>
            <span>training_set = subset(dataset,split == TRUE)</span>
          </p>
          <p><span>testing_set
              = subset(dataset,split == FALSE)</span></p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_19031d3ea5bb5f21.png" name="Imagen 1689703099" align="bottom"
        width="480" height="112" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Escalado de variables</h4>
    <p>No
      se necesita escalado de variables.</p>
    <h4>Crear un modelo de Regresion Lineal Múltiple con
      el conjunto de entrenamiento</h4>
    <p>Usaremos
      una función de regresión lineal indicando en “formula” la
      columna de la variable dependiente y con el punto después de la
      tilde indicamos que el resto de las variables son independientes y en
      data los datos de entrenamiento.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression=lm(formula=Profit ~
              .,data=training_set)</span>
          </p>
          <p>summary(regression)</p>
        
      </tr>
    </table>
    <p>Esto
      es lo que obtenemos, debemos poner atención en la columna <b>Pr</b>
      y ver las que tiene más asteriscos (*) tiene más significatividad,
      según la fila <b>Signif. Codes</b> las primeras variables tienen una
      significatividad de 0.
    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_883d91b9d5861d00.png" name="Imagen 1190684493" align="bottom"
        width="480" height="340" border="0" />
    </p>
    <p>Debemos
      fijarnos en que solo tenemos <b>State1</b> y <b>State2</b>, pero
      tenemos 3 estados la función detecta la multicolinealidad y elimina
      una automáticamente.</p>
    <h4>Predecir con el conjunto de test</h4>
    <p>Mandamos
      el conjunto de testing.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred=predict(regression,newdata=testing_set)</span>
          </p>
          <p>print(y_pred)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h3>Ajustando el modelo de Regresion Lineal Múltiple
      en R (Eliminación hacia atrás)</h3>
    <p><br />
      <br />

    </p>
    <h4>PASO 1: SELECCIONAR EL NIVEL DE SIGNIFICACION</h4>
    <p>Definimos
      el nivel se significación, normalmente es el 0.05.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            SL=0.05</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>PASO 2: SE CALCULA EL MODELO CON TODAS LAS
      POSIBLES VARIABLES PREDICTORAS</h4>
    <p>En
      este momento se están considerando todas las variables predictoras,
      se quita el punto y escribimos una por una todas las variables.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression=lm(formula=Profit ~ R.D.Spend +</span>
          </p>
          <p>
            <span>Administration + </span>
          </p>
          <p>
            <span>Marketing.Spend +</span>
          </p>
          <p>
            <span>State,</span>
          </p>
          <p>
            <span>data=dataset)</span>
          </p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            summary(regression)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_f44c6101b21502ed.png" name="Imagen 1930623916" align="bottom"
        width="480" height="316" border="0" />
    </p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      State &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Se
      elimina <b>State </b>y así eliminamos ambas variables dummy.</p>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression=lm(formula=Profit ~ R.D.Spend +</span>
          </p>
          <p>
            <span>Administration + </span>
          </p>
          <p>
            <span>Marketing.Spend,</span>
          </p>
          <p>
            <span>data=dataset)</span>
          </p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            summary(regression)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_715a2d46f273bec6.png" name="Imagen 1067787075" align="bottom"
        width="480" height="307" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      Administration &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Se
      elimina la variable Administration</p>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression=lm(formula=Profit ~ R.D.Spend + </span>
          </p>
          <p>
            <span>Marketing.Spend,</span>
          </p>
          <p>
            data=dataset)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            summary(regression)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_b113bf813f3e80c8.png" name="Imagen 1057610406" align="bottom"
        width="480" height="273" border="0" />
    </p>
    <p>SI
      P&gt;SL, ENTONCES VAMOS AL PASO 4.</p>
    <p>
      Marketing.Spend &gt; SL, entonces vamos al PASO 4.</p>
    <p>SINO
      AL FIN.</p>
    <h4>PASO 4: SE ELIMINA LA VARIABLE PREDICTORA</h4>
    <p>Se
      elimina la variable Administration</p>
    <h4>PASO 5: AJUSTAR EL MODELO SIN DICHA VARIABLE</h4>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>regression=lm(formula=Profit ~ R.D.Spend ,</span>
          </p>
          <p>
            <span>data=dataset)</span>
          </p>
        
      </tr>
    </table>
    <h4>PASO 3: CONSIDERAR LA VARIABLE PREDICTORA CON EL
      P-VALOR MAS GRANDE.</h4>
    <p>Mostramos
      los resultados.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            summary(regression)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_13e0671824870afa.png" name="Imagen 2023542430" align="bottom"
        width="480" height="275" border="0" />
    </p>
    <p>Ya
      no tenemos mas variables &gt; a SL, lo cual quedamos con la variable
      <b>R.D,Spend</b>.
    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_2cfbc2caaabeb698.png" name="Imagen 1839531377" align="bottom"
        width="457" height="84" border="0" />
    </p>
    <h2>Polinomial</h2>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d0a6c73dabb1c958.png" name="Imagen 2078202025" align="bottom"
        width="359" height="338" border="0" />
    </p>
    <p>Descripción:
      se tienen registrado 10 posiciones salariales, empezando desde el
      puesto más bajo hasta el más alto, en un nivel con sus respectivos
      salarios. Se observa que el aumento del salario es evidente.</p>
    <p>Objetivo:
      predecir un sueldo objetivo en base a los niveles de la posición.</p>
    <p><br />
      <br />

    </p>
    <h3>Python</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset usando pandas. La variable independiente será la columna
      <b>Level</b> y la variable dependiente <b>Salary</b>.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import numpy as np</span>
          </p>
          <p>
            <span>import matplotlib.pyplot as plt</span>
          </p>
          <p>
            <span>import pandas as pd</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>dataset = pd.read_csv('Position_Salaries.csv')</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            X = dataset.iloc[:,1:2].values</p>
          <p>y =
            dataset.iloc[:,2:3].values</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_e255aca5765e9156.png" name="Imagen 279239181" align="bottom"
        width="345" height="408" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>En
      este caso no es recomendable dividir los datos, al ser pocos a simple
      vista se necesita toda la información. En el caso de querer
      dividirlo, el modelo carecerá de mas información.</p>
    <h4>Escalado de variables</h4>
    <p>Como
      tenemos pocos datos, no es necesario escalar variables.</p>
    <h4>Ajustar la Regresion Lineal con el dataset</h4>
    <p>Creamos
      un modelo de regresión lineal para compararlo mas adelante</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.linear_model import
              LinearRegression</span>
          </p>
          <p>
            <span>linear_regression = LinearRegression()</span>
          </p>
          <p><span
             >linear_regression.fit(X,y)</span></p>
        
      </tr>
    </table>
    <h4>Ajustar la Regresion Polinómico con el dataset</h4>
    <p>Creamos
      el modelo de regresión polinómica, a la función le enviamos un
      grado, en este caso le dejaremos a 4, entre mas alto sea el grado el
      modelo se ajustara a los datos. Leer documentación.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.preprocessing import
              PolynomialFeatures</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>poly_regression=PolynomialFeatures(degree=4) #
              se puede jugar con el grado</span>
          </p>
          <p>
            <span>X_poly=poly_regression.fit_transform(X)</span>
          </p>
          <p>
          </p>
        
      </tr>
    </table>
    <p>Lo
      que obtenemos hasta este momento.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_f689daad66dbb16c.png" name="Imagen 10217746" align="bottom"
        width="480" height="262" border="0" />
    </p>
    <p>Hasta
      este momento obtenemos las variables de una ecuación polinómica, en
      la tabla <b>X_poly</b> la columna 0 es la variable independiente, la
      columna 1 es igual a la variable independiente, la columna 2 es el
      cuadrado de la variable independiente y asi sucesivamente.</p>
    <p>Ahora
      pasaremos esos datos polinomiales a la regresion lineal para obtener
      el modelo.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>polynomial_regression=LinearRegression()</span>
          </p>
          <p>
            <span>polynomial_regression.fit(X_poly,y)</span>
          </p>
          <p><br />

          </p>
        
      </tr>
    </table>
    <h4>Visualización de los resultados: Modelo Lineal</h4>
    <p>Visualizamos
      la gráfica de regresion lineal.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>plt.scatter(X,y,color='red')</span>
          </p>
          <p>
            <span>plt.plot(X,linear_regression.predict(X),color='blue')</span>
          </p>
          <p>
            plt.title(&quot;Modelo Regresion Lineal&quot;)</p>
          <p>
            plt.xlabel(&quot;Posicion del empleado&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo en $&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_854cb21ac48f92ec.png" name="Imagen 1760171886" align="bottom"
        width="386" height="278" border="0" />
    </p>
    <p>A
      simple vista nos damos cuenta que el modelo lineal no nos sirve en
      este caso.</p>
    <h4>Visualización de los resultados: Modelo
      Polinómico</h4>
    <p>Visualizamos
      la grafica de regresión polinómica</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>plt.scatter(X,y,color='red')</span>
          </p>
          <p>
            <span>plt.plot(X,
              polynomial_regression.predict(X_poly),color='green')</span>
          </p>
          <p>
            plt.title(&quot;Modelo Regresion Polinomica&quot;)</p>
          <p>
            plt.xlabel(&quot;Posicion del empleado&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo en $&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_cf50f7d08d454d31.png" name="Imagen 1546835576" align="bottom"
        width="386" height="278" border="0" />
    </p>
    <p>Este
      modelo si se ajusta a nuestros datos.</p>
    <h4>Prediccion de los modelos</h4>
    <p>Si
      tenemos un empleado con el nivel 6.5, ¿qué modelo predice mejor?</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>linear_regression.predict([[6.5]])</span>
          </p>
          <p><span
             >polynomial_regression.predict(poly_regression.fit_transform([[6.5]]))</span></p>
        
      </tr>
    </table>
    <p>Por
      supuesto el que mejor se ajusta es el modelo polinómico.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_3311532ef36f3158.png" name="Imagen 1442993617" align="bottom"
        width="480" height="86" border="0" />
    </p>
    <h3>R</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset y tomamos las columnas que necesitaremos.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('Position_Salaries.csv')</span>
          </p>
          <p>dataset=dataset[,2:3]</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_3197499f96a5066c.png" name="Imagen 1413666220" align="bottom"
        width="189" height="270" border="0" />
    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>En
      este caso no es recomendable dividir los datos, al ser pocos a simple
      vista se necesita toda la información. En el caso de querer
      dividirlo, el modelo carecerá de mas información.</p>
    <h4>Escalado de variables</h4>
    <p>Como
      tenemos pocos datos, no es necesario escalar variables.</p>
    <h4>Ajustar la Regresión Lineal con el dataset</h4>
    <p>Llamamos
      a la función <b>lm</b> y en la formula indicamos que <b>Salary</b>
      depende del resto de variables del dataset con la notación (<b>.</b>)
      Y enviamos el conjunto de entrenamiento</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>linear_regressor=lm(formula=Salary ~
              .,data=dataset)</span>
          </p>
          <p>summary(linear_regressor)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_c2620c24d658a241.png" name="Imagen 2086335096" align="bottom"
        width="480" height="257" border="0" />
    </p>
    <h4>Ajustar la Regresión Polinómico con el dataset</h4>
    <p>Se
      vuelve a usar el modelo de regresión lineal, pero construiremos los
      términos de dicho polinomio de forma explícita, agregando columnas
      adicionales, los cuales son los <b>Level</b>
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            dataset$Level2=dataset$Level^2 # agregando esto se modifica el
            valor a polinomico</p>
          <p>
            <span>dataset$Level3=dataset$Level^3</span>
          </p>
          <p>
            <span>dataset$Level4=dataset$Level^4</span>
          </p>
          <p>
            <span>poly_regressor=lm(formula=Salary ~
              .,data=dataset)</span>
          </p>
          <p>summary(poly_regressor)</p>
        
      </tr>
    </table>
    <p>Asi
      se ve el nuevo dataset con las variables un polinomio de grado 4.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_ef79163973350960.png" name="Imagen 1529482489" align="bottom"
        width="425" height="282" border="0" />
    </p>
    <p>Visualizamos
      los resultados.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_22ad1a0a370106d2.png" name="Imagen 770747745" align="bottom"
        width="480" height="302" border="0" />
    </p>
    <h4>Visualización de los resultados: Modelo Lineal</h4>
    <p>Usando
      la librería ggplot.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de entrenamiento</p>
          <p>
            geom_point(aes(x=dataset$Level,y=dataset$Salary),</p>
          <p>
            color=&quot;red&quot;)+</p>
          <p>
            # Dobujando la linea de la prediccion, en base al entrenamiento</p>
          <p>

            <span>geom_line(aes(x=dataset$Level,y=predict(linear_regressor,newdata=dataset)),</span>
          </p>
          <p>
            <span>color=&quot;blue&quot;)+</span>
          </p>
          <p>
            ggtitle(&quot;Prediccion lineal del sueldo en funcion del nivel
            del empleado &quot;)+</p>
          <p>
            xlab(&quot;Nivel empleado&quot;)+</p>
          <p>
            ylab(&quot;Sueldo en $&quot;)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_87d6e4cb8ba2fc0.png" name="Imagen 1908629409" align="bottom"
        width="480" height="261" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Visualización de los resultados: Modelo
      Polinómico</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de entrenamiento</p>
          <p>
            geom_point(aes(x=dataset$Level,y=dataset$Salary),</p>
          <p>
            color=&quot;red&quot;)+</p>
          <p>
            # Dobujando la linea de la prediccion, en base al entrenamiento</p>
          <p>

            <span>geom_line(aes(x=dataset$Level,y=predict(poly_regressor,newdata=dataset)),</span>
          </p>
          <p>
            <span>color=&quot;blue&quot;)+</span>
          </p>
          <p>
            ggtitle(&quot;Prediccion lineal polinomial del sueldo en funcion
            del nivel del empleado &quot;)+</p>
          <p>
            xlab(&quot;Nivel empleado&quot;)+</p>
          <p>
            ylab(&quot;Sueldo en $&quot;)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_12eed17fd448c336.png" name="Imagen 985105097" align="bottom"
        width="480" height="261" border="0" />
    </p>
    <h4>Prediccion de los modelos</h4>
    <p>Comparamos
      los resultados nuevamente, Ss tenemos un empleado con el nivel 6.5,
      ¿qué modelo predice mejor?</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred_linear=predict(linear_regressor,newdata=data.frame(Level=6.5))</span>
          </p>
          <p>
            <span>print(y_pred_linear)</span>
          </p>
          <p>

          </p>
          <p>
            <span>y_pred_poly=predict(poly_regressor,newdata=data.frame(Level=6.5,</span>
          </p>
          <p>

            <span>Level2=6.5^2,</span>
          </p>
          <p>

            <span>Level3=6.5^3,</span>
          </p>
          <p>

            <span>Level4=6.5^4))</span>
          </p>
          <p><span>print(y_pred_poly)</span></p>
        
      </tr>
    </table>
    <p>Por
      supuesto el modelo polinómico.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_155a3a5f4c8e5a16.png" name="Imagen 2122048551" align="bottom"
        width="480" height="186" border="0" />
    </p>
    <h2>Soporte Vectorial</h2>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d0a6c73dabb1c958.png" name="Imagen 728687875" align="bottom"
        width="359" height="338" border="0" />
    </p>
    <p>Descripción:
      se tienen registrado 10 posiciones salariales, empezando desde el
      puesto más bajo hasta el más alto, en un nivel con sus respectivos
      salarios. Se observa que el aumento del salario es evidente.</p>
    <p>Objetivo:
      predecir un sueldo objetivo en base a los niveles de la posición.</p>
    <h3>Python</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset usando pandas. La variable independiente será la columna
      <b>Level</b> y la variable dependiente <b>Salary</b>.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import numpy as np</span>
          </p>
          <p>
            <span>import matplotlib.pyplot as plt</span>
          </p>
          <p>
            <span>import pandas as pd</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>dataset = pd.read_csv('Position_Salaries.csv')</span>
          </p>
          <p>
            X = dataset.iloc[:,1:2].values</p>
          <p>y =
            dataset.iloc[:,2:3].values</p>
        
      </tr>
    </table>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>En
      este caso no es recomendable dividir los datos, al ser pocos a simple
      vista se necesita toda la información. En el caso de querer
      dividirlo, el modelo carecera de más información.</p>
    <h4>Escalado de variables</h4>
    <p>En
      este caso si necesitamos escalar, en caso de omitirlo obtendremos un
      modelo líneal (hacer la prueba omitiendo este paso).</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.preprocessing import
              StandardScaler</span>
          </p>
          <p>

          </p>
          <p>
            <span># Escalador para las variables</span>
          </p>
          <p>
            <span>sc_X = StandardScaler()</span>
          </p>
          <p>
            <span>sc_y = StandardScaler()</span>
          </p>
          <p>

          </p>
          <p>
            <span>X= sc_X.fit_transform(X)</span>
          </p>
          <p>y=
            sc_y.fit_transform(y)</p>
        
      </tr>
    </table>
    <h4>Creando y ajustando el modelo</h4>
    <p>La
      configuración del kernel por defecto es lineal, pero no se ajustará
      a los datos, podría ser un kernel polinómico, pero eso aplicaría
      en el ejemplo anterior y como esta sección el objetivo es unir una
      SVM usaremos un kernel radial.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.svm import SVR</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>svr_regression=SVR(kernel='rbf')</span>
          </p>
          <p><span
             >svr_regression.fit(X,y)</span></p>
        
      </tr>
    </table>
    <h4>Visualización de los resultados
    </h4>
    <p>Mostramos
      la gráfica de regresión polinómico, y se puede ver que tanto la
      curva se acomoda a los datos de entrenamiento.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>plt.scatter(X,y,color='red')</span>
          </p>
          <p>
            <span>plt.plot(X,svr_regression.predict(X),color='green')</span>
          </p>
          <p>
            plt.title(&quot;Modelo Regresion SVR&quot;)</p>
          <p>
            plt.xlabel(&quot;Posicion del empleado&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo en $&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_6c4e3b05eb21a1d8.png" name="Imagen 2121755171" align="bottom"
        width="394" height="278" border="0" />
    </p>
    <h4>Predicción del modelo</h4>
    <p>El
      valor que queremos predecir también se le debe de aplicar el
      escalador de variables para obtener el resultado escalado correcto y
      revertir de nuevo el escalado para obtener el valor correcto.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred_sc=svr_regression.predict(sc_X.transform([[6.5]]))</span>
          </p>
          <p>y_pred=sc_y.inverse_transform([y_pred_sc])</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_dfd9224778a6dda4.png" name="Imagen 1655804554" align="bottom"
        width="477" height="52" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h3>R</h3>
    <p><br />
      <br />

    </p>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('Position_Salaries.csv')</span>
          </p>
          <p>dataset
            = dataset[,2:3]</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_4c52194550920e7e.png" name="Imagen 372774857" align="bottom"
        width="203" height="288" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>En
      este caso no es recomendable dividir los datos, al ser pocos a simple
      vista se necesita toda la información. En el caso de querer
      dividirlo, el modelo carecera de más información.</p>
    <h4>Escalado de variables</h4>
    <p>En
      este caso si necesitamos escalar.</p>
    <h4>Creando y ajustando el modelo</h4>
    <p>La
      configuración del kernel por defecto es lineal, pero no se ajustará
      a los datos, podría ser un kernel polinómico, pero eso aplicaría
      en el ejemplo anterior y como esta sección el objetivo es unir una
      SVM usaremos un kernel radial. En este lenguaje usaremos la funcion
      <b>svm</b> de la librería <b>e1071</b>.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span># library(e1071) </span>
          </p>
          <p>
            <span>svr_regressor=svm(x = dataset$Level,y =
              dataset$Salary,kernel='radial')</span>
          </p>
          <p>summary(svr_regressor)</p>
        
      </tr>
    </table>
    <p>Mostraremos
      unos detalles que ofrece la funcionalidad de <b>svm</b>.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_8a78515986e9dd2c.png" name="Imagen 366206419" align="bottom"
        width="480" height="202" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Visualización de los resultados del modelo</h4>
    <p>Mostramos
      la gráfica de regresión polinómico, y se puede ver que tanto la
      curva se acomoda a los datos de entrenamiento.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de entrenamiento</p>
          <p>
            <span>geom_point(aes(x=dataset$Level,y=dataset$Salary),</span>
          </p>
          <p>
            <span>color=&quot;red&quot;)+</span>
          </p>
          <p>
            # Dobujando la linea de la prediccion, en base al entrenamiento</p>
          <p>

            <span>geom_line(aes(x=dataset$Level,y=predict(svr_regressor,newdata=dataset$Level)),</span>
          </p>
          <p>
            <span>color=&quot;blue&quot;)+</span>
          </p>
          <p>
            <span>ggtitle(&quot;Prediccion SVR &quot;)+</span>
          </p>
          <p>
            <span>xlab(&quot;Nivel del empleado&quot;)+</span>
          </p>
          <p>
            ylab(&quot;Sueldo en $&quot;)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_1d90ff74beec9665.png" name="Imagen 1303706127" align="bottom"
        width="480" height="280" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Prediccion del modelo</h4>
    <p>Evaluamos
      el modelo con un empleado nivel 6.5, en este caso como no escalamos
      la variable a evaluar.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred_type=predict(svr_regressor,newdata=6.5)</span>
          </p>
          <p>print(y_pred_type)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_ce319f5c02e6c8b7.png" name="Imagen 1303952164" align="bottom"
        width="403" height="88" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h2>Arboles de decisión</h2>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d0a6c73dabb1c958.png" name="Imagen 1166232448" align="bottom"
        width="359" height="338" border="0" />
    </p>
    <p>Descripción:
      se tienen registrado 10 posiciones salariales, empezando desde el
      puesto más bajo hasta el mas alto, el un nivel con sus respectivos
      salarios. Se observa que el aumento del salario es evidente.</p>
    <p>Objetivo:
      predecir un sueldo objetivo en base a los niveles de la posición.</p>
    <h3>Python</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset con las columnas a utilizar.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import numpy as np</span>
          </p>
          <p>
            <span>import matplotlib.pyplot as plt</span>
          </p>
          <p>
            <span>import pandas as pd</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>dataset = pd.read_csv('Position_Salaries.csv')</span>
          </p>
          <p>
            X = dataset.iloc[:,1:2].values</p>
          <p>
            y = dataset.iloc[:,2:3].values</p>
          <p><br />

          </p>
        
      </tr>
    </table>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>No
      aplica ya que si necesitamos todos los datos disponibles</p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario</p>
    <h4>Creando y ajustando el modelo</h4>
    <p>Importamos
      el módulo <b>desicionTreeRegressor</b>, para llegar a los mismos
      resultados de este tutorial en <b>random_state </b>le asignamos 0.</p>
    <p>En
      random_state como su nombre indica es un numero cualquiera, solo que
      al usar 0 se obtendrán los mismos resultados a continuación.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.tree import DecisionTreeRegressor</span>
          </p>
          <p>
            <span>tree_regression=DecisionTreeRegressor(random_state=0)</span>
          </p>
          <p><span
             >tree_regression.fit(X,y)</span></p>
        
      </tr>
    </table>
    <h4>Visualización de los resultados</h4>
    <p>Mostraremos
      2 graficas</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>X_grid =np.arange(min(X),max(X),0.1)</span>
          </p>
          <p>
            <span>X_grid=X_grid.reshape(len(X_grid),1)</span>
          </p>
          <p>

          </p>
          <p>
            <span>plt.scatter(X,y,color='red')</span>
          </p>
          <p>
            <span>plt.plot(X_grid,tree_regression.predict(X_grid),color='green')
              # linea 1</span>
          </p>
          <p>
            <span>#
              plt.plot(X,tree_regression.predict(X),color='green') # linea 2</span>
          </p>
          <p>
            plt.title(&quot;Modelo Regresion por arboles de decision&quot;)</p>
          <p>
            plt.xlabel(&quot;Posicion del empleado&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo en $&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p>Comentamos
      la linea 2 y ejecutamos con la linea 1, obtenemos la grafica del
      modelo.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_c3d48e5832ee9375.png" name="Imagen 759240905" align="bottom"
        width="386" height="278" border="0" />
    </p>
    <p>Comentamos
      la linea 1 y ejecutamos con la linea 2, obtenemos la grafica del
      modelo uniendo todos los puntos.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_251b23de4dd9efc6.png" name="Imagen 1821660502" align="bottom"
        width="386" height="278" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h4>Prediccion del modelo</h4>
    <p>Comparamos
      el resultado con la grafica y obtendremos el grupo al que pertenece
      el valor 6.5</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred=tree_regression.predict([[6.5]])</span>
          </p>
          <p>print(y_pred)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_3226913efe0a87d8.png" name="Imagen 1756778231" align="bottom"
        width="425" height="78" border="0" />
    </p>
    <h3>R</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset con las columnas a utilizar.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('Position_Salaries.csv')</span>
          </p>
          <p>dataset
            = dataset[,2:3]</p>
        
      </tr>
    </table>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>No
      aplica ya que si necesitamos todos los datos disponibles.</p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario pocos datos</p>
    <h4>Creando y ajustando el modelo</h4>
    <p>Importamos
      la librería <b>rpart</b> para crear el modelo relacionando <b>Salary</b>
      en función de <b>Level</b>. Enviamos el dataset y en control …</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span># library(rpart) # ejecutar 1 vez</span>
          </p>
          <p>
            <span>tree_regressor=rpart(formula = dataset$Salary ~
              dataset$Level,</span>
          </p>
          <p>
            <span>data=dataset,</span>
          </p>
          <p>
            <span>control =
              rpart.control(minsplit = 1))</span>
          </p>
          <p>summary(tree_regressor)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_a06eca8cd9bfbca0.png" name="Imagen 132007679" align="bottom"
        width="375" height="480" border="0" />
    </p>
    <h4>Visualización de los resultados</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>x_grid=seq(min(dataset$Level),max(dataset$Level),0.1)</span>
          </p>
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de entrenamiento</p>
          <p>
            geom_point(aes(x=dataset$Level,y=dataset$Salary),</p>
          <p>
            color=&quot;red&quot;)+</p>
          <p>
            # Dobujando la linea de la prediccion, en base al entrenamiento</p>
          <p>

            <span>geom_line(aes(x=dataset$Level,y=predict(tree_regressor,newdata=dataset)),</span>
          </p>
          <p>
            <span># geom_line(aes(x=x_grid,</span>
          </p>
          <p>
            <span># y=predict(tree_regressor,</span>
          </p>
          <p>
            # newdata=data.frame(Level=x_grid))), #
            ver porque no funciona esta parte----</p>
          <p>
            color=&quot;blue&quot;)+</p>
          <p>
            ggtitle(&quot;Prediccion usando regresion con arboles de
            decision &quot;)+</p>
          <p>
            xlab(&quot;labelx&quot;)+</p>
          <p>
            ylab(&quot;labely&quot;)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_7bf354a15599c9e4.png" name="Imagen 701104336" align="bottom"
        width="480" height="267" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h4>Prediccion del modelo</h4>
    <p>Comparamos
      el valor 6.5 en la grafica.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred_tree=predict(tree_regressor,newdata=data.frame(Level=6.5))</span>
          </p>
          <p>print(y_pred_tree)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_514d29222fdacc15.png" name="Imagen 1110863431" align="bottom"
        width="480" height="57" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h2>Bosques Aleatorios</h2>
    <h3>Dataset</h3>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_d0a6c73dabb1c958.png" name="Imagen 1509962432" align="bottom"
        width="359" height="338" border="0" />
    </p>
    <p>Descripción:
      se tienen registrado 10 posiciones salariales, empezando desde el
      puesto mas bajo hasta el mas alto, el un nivel con sus respectivos
      salarios. Se observa que el aumento del salario es evidente</p>
    <p><br />
      <br />

    </p>
    <p>Objetivo:
      predecir un sueldo objetivo en base a los niveles de la posición.</p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h3>Python</h3>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <p><br />
      <br />

    </p>
    <h4>Importación del dataset</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>import numpy as np</span>
          </p>
          <p>
            <span>import matplotlib.pyplot as plt</span>
          </p>
          <p>
            <span>import pandas as pd</span>
          </p>
          <p>
            <br />

          </p>
          <p>
            <span>dataset = pd.read_csv('Position_Salaries.csv') </span>
          </p>
          <p>
            <br />

          </p>
          <p>
            X = dataset.iloc[:,1:2].values</p>
          <p>y =
            dataset.iloc[:,2:3].values</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>No
      aplica ya que si necesitamos todos los datos disponibles.</p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario.</p>
    <h4>Creando y ajustando el modelo</h4>
    <p>Importamos
      <b>RandomForestRegressor </b>para crear el modelo y que use 100
      arboles. En <b>random_state</b> igual a 0 para obtener los mismos
      resultados.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>from sklearn.ensemble import
              RandomForestRegressor </span>
          </p>
          <p>
            <span>rf_regression=RandomForestRegressor(n_estimators=100,
              random_state=0)</span>
          </p>
          <p><span>rf_regression.fit(X,y)</span>
          </p>
        
      </tr>
    </table>
    <p>
      <br />
      <br />

    </p>
    <h4>Visualización de los resultados</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>X_grid =np.arange(min(X),max(X),0.1)</span>
          </p>
          <p>
            <span>X_grid=X_grid.reshape(len(X_grid),1)</span>
          </p>
          <p>

          </p>
          <p>
            <span>plt.scatter(X,y,color='red')</span>
          </p>
          <p>
            <span>plt.plot(X_grid,rf_regression.predict(X_grid),color='green')
              #linea1</span>
          </p>
          <p>
            <span>#
              plt.plot(X,rf_regression.predict(X),color='green') #linea2</span>
          </p>
          <p>
            plt.title(&quot;Modelo Regresion por arboles aleatorios de
            decision&quot;)</p>
          <p>
            plt.xlabel(&quot;Posicion del empleado&quot;)</p>
          <p>
            plt.ylabel(&quot;Sueldo en $&quot;)</p>
          <p>plt.show()</p>
        
      </tr>
    </table>
    <p>Ejecución
      con linea 1.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_6609d1744f69c3f9.png" name="Imagen 172250481" align="bottom"
        width="386" height="289" border="0" />
    </p>
    <p>Ejecución
      con linea 2.</p>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_445beca6f30ca7df.png" name="Imagen 865638860" align="bottom"
        width="386" height="289" border="0" />
    </p>
    <h4>Prediccion del modelo</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred=rf_regression.predict([[6.5]])</span>
          </p>
          <p>print(y_pred)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_cc56373153eac65e.png" name="Imagen 2121783912" align="bottom"
        width="372" height="84" border="0" />
    </p>
    <p><br />
      <br />

    </p>
    <h3>R</h3>
    <h4>Importación del dataset</h4>
    <p>Importamos
      el dataset</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>dataset = read.csv('Position_Salaries.csv')</span>
          </p>
          <p>dataset
            = dataset[,2:3]</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Tratamiento de NAs</h4>
    <p>No
      aplica por falta de NAs.</p>
    <h4>Codificar los datos categóricos</h4>
    <p>No
      aplica porque no hay datos por categorizar.</p>
    <h4>Dividiendo el dataset en conjunto de
      entrenamiento y testeo</h4>
    <p>No
      aplica ya que si necesitamos todos los datos disponibles.</p>
    <h4>Escalado de variables</h4>
    <p>No
      es necesario.</p>
    <h4>Creando y ajustando el modelo</h4>
    <p>Importamos
      <b>randomForest,</b> indicamos las variables dependientes e
      independientes y decimos que vamos a usar 100 arboles aleatorios.
    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span># library(randomForest)</span>
          </p>
          <p>
            <span>set.seed(1)</span>
          </p>
          <p>
            <span>rf_regressor=randomForest(x=dataset[1],y=dataset$Salary,</span>
          </p>
          <p>
            <span>ntree=100)</span>
          </p>
          <p>summary(rf_regressor)</p>
        
      </tr>
    </table>
    <p><br />
      <br />

    </p>
    <h4>Visualización de los resultados</h4>
    <p>Mostramos
      la grafica.</p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>x_grid=seq(min(dataset$Level),max(dataset$Level),0.01)</span>
          </p>
          <p>
            # Agregando componentes a mostrar</p>
          <p>
            ggplot()+</p>
          <p>
            # Dobujando los puntos de entrenamiento</p>
          <p>
            geom_point(aes(x=dataset$Level,y=dataset$Salary),</p>
          <p>
            color=&quot;red&quot;)+</p>
          <p>
            # Dobujando la linea de la prediccion, en base al entrenamiento</p>
          <p>
            <span>#
              geom_line(aes(x=dataset$Level,y=predict(rf_regressor,newdata=dataset)),</span>
          </p>
          <p>
            <span>geom_line(aes(x=x_grid,</span>
          </p>
          <p>
            <span>y=predict(rf_regressor,</span>
          </p>
          <p>

            <span>newdata=data.frame(Level=x_grid))),</span>
          </p>
          <p>
            <span>color=&quot;blue&quot;)+</span>
          </p>
          <p>
            ggtitle(&quot;Prediccion usando regresion con arboles aleatorios
            de decision &quot;)+</p>
          <p>
            xlab(&quot;labelx&quot;)+</p>
          <p>
            ylab(&quot;labely&quot;)</p>
        
      </tr>
    </table>
    <p><img
        src="ec2ff694d3eded5ca10a77172ddc40f0_html_3dc74535b2afdf98.png" name="Imagen 1994386304" align="bottom"
        width="480" height="293" border="0" />
    </p>
    <h4>Prediccion del modelo</h4>
    <p><br />
      <br />

    </p>
    <table>
      <col width="573" />

      <tr>
        
          <p>
            <span>y_pred_tree=predict(rf_regressor,newdata=data.frame(Level=6.5))</span>
          </p>
          <p>print(y_pred_tree)</p>
        
      </tr>
    </table>
    
  </main>


  <%- include ../layouts/pie.ejs %>