X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.01)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 500)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier,type = 'class', newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles de Decision (Conjunto de Entrenamiento)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.01)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 500)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier,type = 'class', newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles de Decision (Conjunto de Entrenamiento)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
set = training_set
X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.1)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 250)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier,type = 'class', newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles de Decision (Conjunto de Entrenamiento)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
set = testing_set
X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.1)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 250)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier, type = "class", newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles de Decision (Conjunto de Testing)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
plot(classifier)
text(classifier)
# =============================================================================
# Clasificacion por Arboles de Desicion
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
# Estructura de los datos: {explicar eldataset y el objetivo}.
# Filas :{numero de filas}
# Columnas:
#           |{col1}|{col2}|{...} (vars independiente)
#           |{columna de var indep.}| (var_dependiente)
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[,3:5]
# Codificar la variable de clasificacion como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0,1))
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings-----------------
# =============================================================================
# install.packages("caTools") # solo se necesita ejecutar una vez
# library(caTools)
# configurando semilla aleatoria para la division de datos
set.seed(1)
# se elige el porcentaje de los datos para el training en %
split = sample.split(dataset$Purchased,SplitRatio = 0.25)
#vprint(split)
# Dividiendo el conjunto , False para el test
training_set = subset(dataset,split == FALSE)
# Dividiendo el conjunto , True para el training
testing_set = subset(dataset,split == TRUE)
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# no es necesario
# =============================================================================
# Ajustar el modelo de {  } al conjunto de entrenamiento
# =============================================================================
# install.packages("randomForest") # solo se necesita ejecutar una vez
# library(randomForest)
classifier =randomForest(x=training_set[,-3],
y=training_set$Purchased,
ntree=10)
# =============================================================================
# Prediccion de los resultados con el conjunto de testing
# =============================================================================
# obtenemos las probabilidades listadas
y_pred = predict(classifier, newdata = testing_set[,-3])
print(y_pred)
# =============================================================================
# Elaborar una Matriz de confusion
#
# |----------------------|----------------------|
# |     Verdaderos       |      Falsos          |
# |     Positivo         |      Positivos       |
# |----------------------|----------------------|
# |     Falsos           |      Verdaderos      |
# |     Negativos        |      Negativos       |
# |----------------------|----------------------|
# =============================================================================
cm = table(testing_set[,3],y_pred)
print(cm)
# =============================================================================
# Representacion grafica de los resultados del modelo (Entrenamiento)
# =============================================================================
# install.packages("ElemStatLearn") # solo se necesita ejecutar una vez
# library(ElemStatLearn)
set = training_set
X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.1)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 250)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier,type = 'class', newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles Aleatorios(Conjunto de Entrenamiento)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
# =============================================================================
# Representacion grafica de los resultados del modelo (Testing)
# =============================================================================
set = testing_set
X1 = seq(min(set[,1]) -1, max(set[,1]) + 1, by = 0.1)
X2 = seq(min(set[,2]) -1, max(set[,2]) + 1, by = 250)
grid_set = expand.grid(X1,X2)
colnames(grid_set) = c('Age','EstimatedSalary')
y_grid =predict(classifier, type = "class", newdata = grid_set)
plot(
set[,-3],
main = "Clasificacion Arboles Aleatorios (Conjunto de Testing)",
xlab = 'Edad',
ylab = 'Sueldo Estimado',
xlim = range(X1),
ylim = range(X2))
contour(
X1,
X2,
matrix(as.numeric(y_grid), length(X1), length(X2)),
add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[,3] == 1, 'green4', 'red3'))
print(cm)
# =============================================================================
# K-Means
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
setwd("~/Documentos/Proyectos_IA/MachineLearningAZ/4-Clustering/1-K-Means_Clustering")
# =============================================================================
# K-Means
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
X=dataset[,4:5]
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Tratamiendo de NAs--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Codificar datos categoricos--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# No aplica
# =============================================================================
# ---------Metodo del codo para averiguarl el numero optimo de clusters-------
# =============================================================================
set.seed(0)
wcss=vector()
for(i in 1:10){
wcss[i] <- sum(kmeans(X,i)$withinss )
}
print(wcss)
plot( 1:10, wcss, type = 'b', main= "Metodo del codo",
xlab = "Numero de clusters K",
ylab = "WCSS K")
# =============================================================================
# ----------Aplicando el metodo Kmeans para segmentar el dataset---------
# =============================================================================
# Con ayuda de la grafica anterior sabemos el el k = 5 es el optimo
k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ---------------Visualizacionde los clusters------------
# =============================================================================
# library(cluster)
clusplot(X, k_means$cluster, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
clusplot(X, k_means$cluster, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
library(cluster, lib.loc = "/usr/lib/R/library")
k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ---------------Visualizacionde los clusters------------
# =============================================================================
# library(cluster)
clusplot(X, k_means$cluster, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
# =============================================================================
# K-Means
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
X=dataset[,4:5]
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Tratamiendo de NAs--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Codificar datos categoricos--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# No aplica
# =============================================================================
# ---------Metodo del codo para averiguarl el numero optimo de clusters-------
# =============================================================================
set.seed(0)
wcss=vector()
for(i in 1:10){
wcss[i] <- sum(kmeans(X,i)$withinss )
}
print(wcss)
plot( 1:10, wcss, type = 'b', main= "Metodo del codo",
xlab = "Numero de clusters K",
ylab = "WCSS K")
# =============================================================================
# ----------Aplicando el metodo Kmeans para segmentar el dataset---------
# =============================================================================
# Con ayuda de la grafica anterior sabemos el el k = 5 es el optimo
k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ---------------Visualizacionde los clusters------------
# =============================================================================
# library(cluster)
clusplot(X, k_means$cluster, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
# =============================================================================
# Clustering Jerarquico
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
X=dataset[,4:5]
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Tratamiendo de NAs--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Codificar datos categoricos--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# No aplica
# =============================================================================
# ----Utilizando Dendrogramas para encontrar el numero optimo de cluster--
# =============================================================================
dendrogram=hclust(dist(X,method = "euclidean"),
method = "ward.D")
plot(dendrogram,
main = "Dendrograma",
xlab = "clientes del centro comercial",
ylab = "distancia euclidea")
# =============================================================================
# ----------Aplicando el metodo Kmeans para segmentar el dataset---------
# =============================================================================
# Con ayuda de la grafica anterior sabemos el el k = 5 es el optimo
k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ----Ajustando el clustering jerarquico a los datos----
# =============================================================================
hc=hclust(dist(X,method = "euclidean"),
method = "ward.D")
y_hc = cutree(hc,k=5,)
# =============================================================================
# Visualizacion de los datos
# =============================================================================
# library(cluster)
clusplot(X, y_hc, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
# =============================================================================
# Clustering Jerarquico
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
X=dataset[,4:5]
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Tratamiendo de NAs--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Codificar datos categoricos--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# No aplica
# =============================================================================
# ----Utilizando Dendrogramas para encontrar el numero optimo de cluster--
# =============================================================================
dendrogram=hclust(dist(X,method = "euclidean"),
method = "ward.D")
plot(dendrogram,
main = "Dendrograma",
xlab = "clientes del centro comercial",
ylab = "distancia euclidea")
# =============================================================================
# ----------Aplicando el metodo Kmeans para segmentar el dataset---------
# =============================================================================
# Con ayuda de la grafica anterior sabemos el el k = 5 es el optimo
k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ----Ajustando el clustering jerarquico a los datos----
# =============================================================================
hc=hclust(dist(X,method = "euclidean"),
method = "ward.D")
y_hc = cutree(hc,k=5,)
# =============================================================================
# Visualizacion de los datos
# =============================================================================
# library(cluster)
clusplot(X, y_hc, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
# =============================================================================
# Clustering Jerarquico
# =============================================================================
# =============================================================================
# --------------------Importando dataset--------------------
# =============================================================================
dataset = read.csv('Mall_Customers.csv')
X=dataset[,4:5]
# No se hace ninguna distincion entre variables independientes
# y variables dependientes en R
# =============================================================================
# --------------------Tratamiendo de NAs--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Codificar datos categoricos--------------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Dividiendo dataset en conjuntos--------------------
# --------------------de entrenamiento y conjunto de testings------------
# =============================================================================
# No aplica
# =============================================================================
# --------------------Escalado de variables--------------------
# =============================================================================
# No aplica
# =============================================================================
# ----Utilizando Dendrogramas para encontrar el numero optimo de cluster--
# =============================================================================
dendrogram=hclust(dist(X,method = "euclidean"),
method = "ward.D")
plot(dendrogram,
main = "Dendrograma",
xlab = "clientes del centro comercial",
ylab = "distancia euclidea")
# =============================================================================
# ----------Aplicando el metodo Kmeans para segmentar el dataset---------
# =============================================================================
# Con ayuda de la grafica anterior sabemos el el k = 5 es el optimo
# k_means=kmeans(X,5,iter.max = 300, nstart = 10)
# =============================================================================
# ----Ajustando el clustering jerarquico a los datos----
# =============================================================================
hc=hclust(dist(X,method = "euclidean"),
method = "ward.D")
y_hc = cutree(hc,k=5,)
# =============================================================================
# Visualizacion de los datos
# =============================================================================
# library(cluster)
clusplot(X, y_hc, lines = 0, shade = TRUE,
color = TRUE,
labels = 4,
plotchar = FALSE,
span = TRUE,
main = "Clustering de clientes",
xlab = "Ingresos anuales",
ylab = "Puntuacion")
